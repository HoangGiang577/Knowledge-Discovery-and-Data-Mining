{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Task3.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeNTCKnKnN6R"
      },
      "source": [
        "# I.Import Library\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AN8rgbmrdKJP"
      },
      "source": [
        "\n",
        "%tensorflow_version 1.x\n",
        "!pip install keras==2.2.5\n",
        "!pip install pyvi\n",
        "\n",
        "import numpy as np\n",
        "from numpy import random\n",
        "import os, pickle, re, keras, sklearn, string\n",
        "from keras.callbacks import *\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from pyvi import ViTokenizer, ViPosTagger\n",
        "from keras.layers import *\n",
        "from keras.models import Model\n",
        "from keras import optimizers\n",
        "import gensim, operator, json\n",
        "import pandas as pd\n",
        "from sklearn.metrics import *\n",
        "import keras.backend as K\n",
        "from keras.models import *\n",
        "from keras import initializers, regularizers\n",
        "from keras import optimizers\n",
        "from keras.engine.topology import Layer\n",
        "from keras import constraints"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqqjA5rmnXW9"
      },
      "source": [
        "# II.Read Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_QyBjmaFumz"
      },
      "source": [
        "!wget https://thiaisotajppub.s3-ap-northeast-1.amazonaws.com/publicfiles/baomoi.model.bin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ww3kuJBNFyT_"
      },
      "source": [
        "# !wget https://github.com/nthanhkhang/Vietnamese-Social-Media-Emotion-Corpus/raw/main/UIT-VSMEC.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6I-G2B7CwyA"
      },
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile(\"UIT-VSMEC.zip\",\"r\") as zf:\n",
        "    zf.extractall()\n",
        "print(zf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rp1-Qa_81FEY"
      },
      "source": [
        "path_train ='comments_Task1.csv'\n",
        "path_valid ='data/valid_nor_811.csv'\n",
        "path_test ='data/test_nor_811.csv'\n",
        "path_stopword = 'data/stopwords.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3JUFIaAndQo"
      },
      "source": [
        "# III.Word2vec using baomoi.model.bin\n",
        "\n",
        "*   Function reading pretrain word embedding library.\n",
        "*   The word embedding pretrain has been trained in new news, 300-way news\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Skg7lUZ9LYAn"
      },
      "source": [
        "path_embedding= 'baomoi.model.bin'\n",
        "\n",
        "import io\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "word_embedding = KeyedVectors.load_word2vec_format(path_embedding, binary=True)\n",
        "# Example of taking vector of 1 word in the word embedding pretrain\n",
        "EMBEDDING_DIM = word_embedding['yêu'].shape[0]\n",
        "print(\"Embedding: \",EMBEDDING_DIM)\n",
        "# Vector of love words in pretrained word embedding set.\n",
        "print(word_embedding['yêu'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lo7k112MoVAQ"
      },
      "source": [
        "# IV. Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5Gb-4LVo1bf"
      },
      "source": [
        "## 1.Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWTniSb1o0d9"
      },
      "source": [
        "def tokenizer(text):\n",
        "    token = ViTokenizer.tokenize(text)\n",
        "    return token"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsAfz-Kco86j"
      },
      "source": [
        "## 2.Delete Icon"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCchYWSMo9y5"
      },
      "source": [
        "def deleteIcon(text):\n",
        "    text = text.lower()\n",
        "    s = ''\n",
        "    pattern = r\"[a-zA-ZaăâbcdđeêghiklmnoôơpqrstuưvxyàằầbcdđèềghìklmnòồờpqrstùừvxỳáắấbcdđéếghíklmnóốớpqrstúứvxýảẳẩbcdđẻểghỉklmnỏổởpqrstủửvxỷạặậbcdđẹệghịklmnọộợpqrstụựvxỵãẵẫbcdđẽễghĩklmnõỗỡpqrstũữvxỹAĂÂBCDĐEÊGHIKLMNOÔƠPQRSTUƯVXYÀẰẦBCDĐÈỀGHÌKLMNÒỒỜPQRSTÙỪVXỲÁẮẤBCDĐÉẾGHÍKLMNÓỐỚPQRSTÚỨVXÝẠẶẬBCDĐẸỆGHỊKLMNỌỘỢPQRSTỤỰVXỴẢẲẨBCDĐẺỂGHỈKLMNỎỔỞPQRSTỦỬVXỶÃẴẪBCDĐẼỄGHĨKLMNÕỖỠPQRSTŨỮVXỸ,._]\"\n",
        "    for char in text:\n",
        "        if char !=' ':\n",
        "            if len(re.findall(pattern, char)) != 0:\n",
        "                s+=char\n",
        "            elif char == '_':\n",
        "                s+=char\n",
        "        else:\n",
        "            s+=char\n",
        "    s = re.sub('\\\\s+',' ',s)\n",
        "    return s.strip()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5xJDr9epIEJ"
      },
      "source": [
        "## 3.Clean Doc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1_TFsm9WdA0"
      },
      "source": [
        "def clean_doc(doc):\n",
        "    doc = tokenizer(doc)\n",
        "    for punc in string.punctuation:# delete all punctuation (!,? ..) in a sentence\n",
        "        if punc != \"_\":\n",
        "            doc = doc.replace(punc,' ')\n",
        "    doc = deleteIcon(doc) \n",
        "    doc = re.sub(r\"[0-9]+\", \" num \", doc)# Delete numbers\n",
        "    doc = doc.lower()#lowercase \n",
        "    doc = re.sub('\\\\s+',' ',doc)# Remove lots of spaces\n",
        "    return doc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJlgGwaxudQc"
      },
      "source": [
        "## 4.Stopword"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPeiHFV2vRMY"
      },
      "source": [
        "# from underthesea import word_tokenize\n",
        "def pre_process(questions):\n",
        "    stop_words = stopwords.words(\"english\")\n",
        "    questions_stop = [[t for t in tokens if (t not in stop_words) and (3 < len(t.strip()) < 15)]\n",
        "                      for tokens in questions_tokens]\n",
        "    questions_stop = pd.Series(questions_stop)\n",
        "    return questions_stop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpylx70bxTGD"
      },
      "source": [
        "## 5.Word Segmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmCkYJpypwlS"
      },
      "source": [
        "# V.Train/Test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Usy-4ygPWdA2"
      },
      "source": [
        "train_data = pd.read_csv(path_train)\n",
        "valid_data = pd.read_csv(path_valid)\n",
        "test_data = pd.read_csv(path_test)\n",
        "\n",
        "X_train = train_data[\"Sentence\"].apply(lambda x : clean_doc(x))\n",
        "y_train = train_data[\"Emotion\"]\n",
        "\n",
        "X_val = valid_data[\"Sentence\"].apply(lambda x : clean_doc(x))\n",
        "y_val = valid_data[\"Emotion\"]\n",
        "\n",
        "X_test = test_data[\"Sentence\"].apply(lambda x : clean_doc(x))\n",
        "y_test = test_data[\"Emotion\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnYcGTbL0jnu"
      },
      "source": [
        "print(len(X_train),len(y_train))\n",
        "print(len(X_val),len(y_val))\n",
        "print(len(X_test),len(y_test))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyPnd4f4qDSJ"
      },
      "source": [
        "## 1.Catalog vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyIQeZi-WdA4"
      },
      "source": [
        "classes = ['Anger','Disgust','Enjoyment','Fear','Other','Sadness','Surprise']\n",
        "def to_category_vector(label):\n",
        "    vector = np.zeros(len(classes)).astype(np.float64)\n",
        "    index = classes.index(label)\n",
        "    vector[index] = 1.0\n",
        "    return vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ppniGtjqfhg"
      },
      "source": [
        "## 2.Convert labels to numbers in train and test practice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqYyTPmLWdA_"
      },
      "source": [
        "y_train_encode = []\n",
        "for label in y_train:\n",
        "    y_train_encode.append(to_category_vector(label))\n",
        "\n",
        "\n",
        "y_val_encode = []\n",
        "for label in y_val:\n",
        "    y_val_encode.append(to_category_vector(label))\n",
        "\n",
        "print(classes)\n",
        "print(y_train_encode[0])\n",
        "print(y_train[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hK6nyuqOq171"
      },
      "source": [
        "## 3.LSTM\n",
        "\n",
        "\n",
        "*   All the words in the X_train set will form a dictionary\n",
        "*   Each vector of the input word, it will turn into a vector with a fixed number of dimensions and each vocabulary will be replaced by its index in the dictionary\n",
        "* Number of vector dimensions per input we will take the longest sentence which is the direction of the vector and the shorter arcs will automatically add the value 0 after"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EB_NYPlWdBC"
      },
      "source": [
        "xLengths = [len(x.split(' ')) for x in X_train]\n",
        "h = sorted(xLengths)  #sorted lengths\n",
        "maxLength =h[len(h)-1]\n",
        "print(\"The longest sentence length value: \",maxLength)\n",
        "input_tokenizer = Tokenizer(filters=\"\",oov_token=\"UNK\")\n",
        "input_tokenizer.fit_on_texts(X_train)\n",
        "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
        "word_index = input_tokenizer.word_index\n",
        "print(\"input_vocab_size:\",input_vocab_size)\n",
        "X_train_encode = np.array(pad_sequences(input_tokenizer.texts_to_sequences(X_train), maxlen=maxLength,padding=\"post\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rA3sXud4rkcS"
      },
      "source": [
        "## 4.Enter the example using LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxIekrnfrfBg"
      },
      "source": [
        "print(\"Input String : \", X_train[0])\n",
        "print(\"Encode : \",X_train_encode[0])\n",
        "\n",
        "X_val_encode = np.array(pad_sequences(input_tokenizer.texts_to_sequences(X_val), maxlen=maxLength,padding=\"post\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzqNhkCQr7gr"
      },
      "source": [
        "## 5.Generate Embedding\n",
        "Function takes the vector of vocabulary in pre-trained word embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Td-ML6bqWdBE"
      },
      "source": [
        "def generate_embedding(word_index, model_embedding,EMBEDDING_DIM):\n",
        "    count6 = 0\n",
        "    countNot6 = 0\n",
        "    #embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM)) \n",
        "    embedding_matrix = np.asarray([np.random.uniform(-0.01,0.01,EMBEDDING_DIM) for _ in range((len(word_index) + 1))])\n",
        "    list_oov = []\n",
        "    word_is_trained = []\n",
        "    for word, i in word_index.items():\n",
        "        try:\n",
        "            embedding_vector = model_embedding[word]\n",
        "            word_is_trained.append(word)\n",
        "        except:\n",
        "            continue\n",
        "        if embedding_vector is not None:\n",
        "            count6 +=1\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "    \n",
        "    print('Number of words in pre-train embedding: ' + str(count6))\n",
        "    print('Number of words not in pre-train embedding: ' + str(countNot6))\n",
        "    return embedding_matrix,word_is_trained"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRIghLqMBuhV"
      },
      "source": [
        "embedding_matrix,word_is_trained = generate_embedding(word_index,word_embedding,EMBEDDING_DIM)\n",
        "print(word_is_trained)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjvoCKFAsKEK"
      },
      "source": [
        "# VI.Attention Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oYeHeBL-wqO"
      },
      "source": [
        "def dot_product(x, kernel):\n",
        "\tif K.backend() == 'tensorflow':\n",
        "\t\treturn K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
        "\telse:\n",
        "\t\treturn K.dot(x, kernel)\n",
        "\n",
        "class AttentionWithContext(Layer):\n",
        "\tdef __init__(self,\n",
        "\t\t\t\t W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
        "\t\t\t\t W_constraint=None, u_constraint=None, b_constraint=None,\n",
        "\t\t\t\t bias=True, **kwargs):\n",
        "\n",
        "\t\tself.supports_masking = True\n",
        "\t\tself.init = initializers.get('glorot_uniform')\n",
        "\n",
        "\t\tself.W_regularizer = regularizers.get(W_regularizer)\n",
        "\t\tself.u_regularizer = regularizers.get(u_regularizer)\n",
        "\t\tself.b_regularizer = regularizers.get(b_regularizer)\n",
        "\n",
        "\t\tself.W_constraint = constraints.get(W_constraint)\n",
        "\t\tself.u_constraint = constraints.get(u_constraint)\n",
        "\t\tself.b_constraint = constraints.get(b_constraint)\n",
        "\n",
        "\t\tself.bias = bias\n",
        "\t\tsuper(AttentionWithContext, self).__init__(**kwargs)\n",
        "\n",
        "\tdef build(self, input_shape):\n",
        "\t\tassert len(input_shape) == 3\n",
        "\n",
        "\t\tself.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
        "\t\t\t\t\t\t\t\t initializer=self.init,\n",
        "\t\t\t\t\t\t\t\t name='{}_W'.format(self.name),\n",
        "\t\t\t\t\t\t\t\t regularizer=self.W_regularizer,\n",
        "\t\t\t\t\t\t\t\t constraint=self.W_constraint)\n",
        "\t\tif self.bias:\n",
        "\t\t\tself.b = self.add_weight((input_shape[-1],),\n",
        "\t\t\t\t\t\t\t\t\t initializer='zero',\n",
        "\t\t\t\t\t\t\t\t\t name='{}_b'.format(self.name),\n",
        "\t\t\t\t\t\t\t\t\t regularizer=self.b_regularizer,\n",
        "\t\t\t\t\t\t\t\t\t constraint=self.b_constraint)\n",
        "\n",
        "\t\tself.u = self.add_weight((input_shape[-1],),\n",
        "\t\t\t\t\t\t\t\t initializer=self.init,\n",
        "\t\t\t\t\t\t\t\t name='{}_u'.format(self.name),\n",
        "\t\t\t\t\t\t\t\t regularizer=self.u_regularizer,\n",
        "\t\t\t\t\t\t\t\t constraint=self.u_constraint)\n",
        "\n",
        "\t\tsuper(AttentionWithContext, self).build(input_shape)\n",
        "\n",
        "\tdef compute_mask(self, input, input_mask=None):\n",
        "\t\t# do not pass the mask to the next layers\n",
        "\t\treturn None\n",
        "\n",
        "\tdef call(self, x, mask=None):\n",
        "\t\tuit = dot_product(x, self.W)\n",
        "\n",
        "\t\tif self.bias:\n",
        "\t\t\tuit += self.b\n",
        "\n",
        "\t\tuit = K.tanh(uit)\n",
        "\t\tait = dot_product(uit, self.u)\n",
        "\n",
        "\t\ta = K.exp(ait)\n",
        "\n",
        "\t\tif mask is not None:\n",
        "\t\t\ta *= K.cast(mask, K.floatx())\n",
        "\t\ta /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "\n",
        "\t\ta = K.expand_dims(a)\n",
        "\t\tweighted_input = x * a\n",
        "\t\t\n",
        "\t\treturn weighted_input\n",
        "\n",
        "\tdef compute_output_shape(self, input_shape):\n",
        "\t\treturn input_shape[0], input_shape[1], input_shape[2]\n",
        "\t\n",
        "class Addition(Layer):\n",
        "\tdef __init__(self, **kwargs):\n",
        "\t\tsuper(Addition, self).__init__(**kwargs)\n",
        "\n",
        "\tdef build(self, input_shape):\n",
        "\t\tself.output_dim = input_shape[-1]\n",
        "\t\tsuper(Addition, self).build(input_shape)\n",
        "\n",
        "\tdef call(self, x):\n",
        "\t\treturn K.sum(x, axis=1)\n",
        "\n",
        "\tdef compute_output_shape(self, input_shape):\n",
        "\t\treturn (input_shape[0], self.output_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrCQ522csUQ0"
      },
      "source": [
        "## 1.Build mode LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8b-4SQdfzFQ8"
      },
      "source": [
        "filter_nums = 256 # best 128\n",
        "def build_model():\n",
        "        inputs  = Input(shape=(maxLength, ), dtype='float64', name='inputs')    \n",
        "        embedding_layer = Embedding(input_vocab_size,EMBEDDING_DIM,weights=[embedding_matrix], input_length=maxLength, trainable=True,name = 'word_emb')(inputs)\n",
        "        embedding_layer = SpatialDropout1D(0.75)(embedding_layer)\n",
        "                \n",
        "              \n",
        "        lstm_feature1 = CuDNNLSTM(filter_nums, return_sequences=True)(embedding_layer)\n",
        "\n",
        "        att1 = AttentionWithContext()(lstm_feature1)\n",
        "        att1 = Addition()(att1)\n",
        "\n",
        "        fc1 = Dropout(0.5)(Dense(256, name = 'dense_1')(att1))\n",
        "        output1 = Dense(len(classes),name=\"output1\", activation='softmax')(fc1)\n",
        "\n",
        "    \n",
        "        # define optimizer\n",
        "\n",
        "        model = Model(inputs=inputs, outputs=output1)\n",
        "        tensorBoardCallback = TensorBoard(log_dir='./logs', write_graph=True)\n",
        "        \n",
        "        model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "        \n",
        "        history = model.fit(X_train_encode, np.array(y_train_encode), validation_data = (X_val_encode,np.array(y_val_encode)) , batch_size=50, epochs=100,callbacks=[tensorBoardCallback])\n",
        "        return model\n",
        "\n",
        "model = build_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MmCoU4ksp67"
      },
      "source": [
        "## 2.Predict the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQwJ75p_WdBJ"
      },
      "source": [
        "X_test_encode = np.array(pad_sequences(input_tokenizer.texts_to_sequences(X_test), maxlen=maxLength,padding=\"post\"))\n",
        "test_length = len(X_test_encode)\n",
        "\n",
        "y_predict = []\n",
        "predicted = model.predict(X_test_encode)\n",
        "for predict in predicted:\n",
        "    index2, value = max(enumerate(predict), key=operator.itemgetter(1))\n",
        "    y_predict.append(classes[index2])\n",
        "    \n",
        "print(y_predict[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "CWnvBNeI0joN"
      },
      "source": [
        "print(y_predict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VXZOPGcs9O_"
      },
      "source": [
        "## 3.Report the performance metrics (Accuracy, F1-score...)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoEymwgf0joR"
      },
      "source": [
        "precision = precision_score(y_test, y_predict, average='weighted')\n",
        "recall = recall_score(y_test, y_predict, average='weighted')\n",
        "f1score = f1_score(y_test, y_predict, average='micro')\n",
        "accuracy = accuracy_score(y_test, y_predict)\n",
        "\n",
        "print(\"Result model LSTM + Attention layer\")\n",
        "print(\"Results of the models\")\n",
        "print(\"Precision: \", precision)\n",
        "print(\"Recall: \", recall)\n",
        "print(\"F1-Score: \", f1score)\n",
        "print(\"Accuracy: \", accuracy)\n",
        "\n",
        "print(classification_report(y_test,y_predict))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2pll3z0tLDW"
      },
      "source": [
        "# VII.Enter the demo program into 1 sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WyndWU_0joU"
      },
      "source": [
        "def demo(str):\n",
        "  demo_pre = clean_doc(str)\n",
        "  X_demo_encode = np.array(pad_sequences(input_tokenizer.texts_to_sequences([demo_pre]), maxlen=maxLength,padding=\"post\"))\n",
        "  predicted = model.predict(X_demo_encode)\n",
        "  index2, value = max(enumerate(predicted[0]), key=operator.itemgetter(1))\n",
        "  # print(\"Predict the results:\", classes[index2])\n",
        "  return classes[index2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVJw8Vi5thbF"
      },
      "source": [
        "demo('ông JTT làm nỗi da dịch luôn á chời , đỉnh ^ ^')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nb0I3AlEIG8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}